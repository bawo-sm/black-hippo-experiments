{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5fbeac",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6196bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILEPATH = \"../resources/train_df.csv\"\n",
    "TEST_DATA_FILEPATH = \"../resources/test_df.csv\"\n",
    "OPENAI_FILEPATH = \"../openai_key.txt\"\n",
    "LLM = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e73c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(TEST_DATA_FILEPATH)\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b40e8",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt_for_category(client: OpenAI, prompt: str, categories: list[str]) -> dict[str, str]:\n",
    "    response = client.responses.create(\n",
    "        model=LLM,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        max_tool_calls=1,\n",
    "        tools=[{\n",
    "            \"name\": \"result\",\n",
    "            \"type\": \"function\",\n",
    "            \"description\": \"Use this tool to format answer\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100},\n",
    "                    \"explanation\": {\"type\": \"string\"},\n",
    "                    \"category\": {\"type\": \"string\", \"enum\": categories},\n",
    "                },\n",
    "                \"required\": [\"confidence\", \"explanation\", \"category\"],\n",
    "            },\n",
    "            \"additionalProperties\": False\n",
    "        }]\n",
    "    )\n",
    "    return json.loads(response.output[0].arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_text_input(df: pd.DataFrame, i: int) -> str:\n",
    "    text = f\"\"\"\n",
    "    Supplier name = {df[\"supplier_name\"].iloc[i]}\n",
    "    Product name = {df[\"supplier_reference_description\"].iloc[i]}\n",
    "    Product price = {df[\"purchase_price\"].iloc[i]}\n",
    "    Materials = {df[\"materials\"].iloc[i]}\n",
    "    \"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66681b",
   "metadata": {},
   "source": [
    "# Prompts templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01133d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_0_template = \"\"\"\n",
    "<system>\n",
    "    You are an assitant that helps with product categorization. \n",
    "    Products are typically realted with house, home, garden items.\n",
    "</system>\n",
    "\n",
    "\n",
    "<task>\n",
    "    1. Analyze the given product described in a \"product\" tag. \n",
    "    2. Try to find the best category in the \"categories\" tag.\n",
    "</task>\n",
    "\n",
    "\n",
    "<higher-class>\n",
    "    {higher_class}\n",
    "</higher-class>\n",
    "\n",
    "\n",
    "<product>\n",
    "    {product}\n",
    "</product>\n",
    "\n",
    "\n",
    "<categories>\n",
    "    {categories}\n",
    "</categories>\n",
    "\n",
    "\n",
    "<answer-schema>\n",
    "    1. confidence: how much are you sure of your answer? 0 means totally not sure, near 50 means you are bit sure and bit not sure, 100 means that you are totally sure.\n",
    "    2. expalnation: a very short explanation of your choice\n",
    "    3. category: the choosen category\n",
    "</answer-schema>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca6147",
   "metadata": {},
   "source": [
    "# Classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = (\n",
    "    pd.concat([test_df, train_df])\n",
    "    .groupby([\"main\", \"sub\", \"detail\", \"level4\"])\n",
    "    .item_id\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .drop(\"item_id\", axis=1)\n",
    ")\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4a217",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ad093",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=open(OPENAI_FILEPATH, \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "n = 20\n",
    "test_df = test_df.sample(len(test_df))\n",
    "\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    test_row = row_to_text_input(test_df, i)\n",
    "\n",
    "    # main\n",
    "    higher_class = \"Now, you generate the highest general level classification.\"\n",
    "    categories = list(tree.main.unique())\n",
    "    if len(categories) > 1:\n",
    "        prompt = prompt_0_template.format(product=test_row, categories=categories, higher_class=higher_class)\n",
    "        main_class = ask_gpt_for_category(openai_client, prompt, categories)['category']\n",
    "    else:\n",
    "        main_class = categories[0]\n",
    "    \n",
    "    # sub\n",
    "    higher_class = f\"The general classification is {main_class}\"\n",
    "    categories = list(tree[tree.main == main_class][\"sub\"].unique())\n",
    "    if len(categories) > 1:\n",
    "        prompt = prompt_0_template.format(product=test_row, categories=categories, higher_class=higher_class)\n",
    "        sub_class = ask_gpt_for_category(openai_client, prompt, categories)['category']\n",
    "    else:\n",
    "        sub_class = categories[0]\n",
    "\n",
    "    # detail\n",
    "    higher_class = f\"The general classifcation is {main_class}/{sub_class}\"\n",
    "    categories = list(tree[(tree.main == main_class) & (tree[\"sub\"] == sub_class)][\"detail\"].unique())\n",
    "    if len(categories) > 1:\n",
    "        prompt = prompt_0_template.format(product=test_row, categories=categories, higher_class=higher_class)\n",
    "        detail_class = ask_gpt_for_category(openai_client, prompt, categories)['category']\n",
    "    else:\n",
    "        detail_class = categories[0]\n",
    "\n",
    "    # level4\n",
    "    higher_class = f\"The general classifcation is {main_class}/{sub_class}/{detail_class}\"\n",
    "    categories = list(tree[(tree.main == main_class) & (tree[\"sub\"] == sub_class) & (tree[\"detail\"] == detail_class)][\"level4\"].unique())\n",
    "    if len(categories) > 1:\n",
    "        prompt = prompt_0_template.format(product=test_row, categories=categories, higher_class=higher_class)\n",
    "        level4_class = ask_gpt_for_category(openai_client, prompt, categories)['category']\n",
    "    else:\n",
    "        level4_class = categories[0]\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        \"item_id\": test_df[\"item_id\"].iloc[i],\n",
    "        \"main\": main_class,\n",
    "        \"sub\": sub_class,\n",
    "        \"detail\": detail_class,\n",
    "        \"level4\": level4_class\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_score, sub_score, detail_score, level4_score, total_score = 0, 0, 0, 0, 0\n",
    "\n",
    "for i in range(len(results_df)):\n",
    "    success = 0\n",
    "\n",
    "    if results_df[\"main\"].iloc[i] == test_df[\"main\"].iloc[i]:\n",
    "        main_score += 1\n",
    "        success += 1\n",
    "    \n",
    "    if results_df[\"sub\"].iloc[i] == test_df[\"sub\"].iloc[i]:\n",
    "        sub_score += 1\n",
    "        success += 1\n",
    "\n",
    "    if results_df[\"detail\"].iloc[i] == test_df[\"detail\"].iloc[i]:\n",
    "        detail_score += 1\n",
    "        success += 1\n",
    "\n",
    "    if results_df[\"level4\"].iloc[i] == test_df[\"level4\"].iloc[i]:\n",
    "        level4_score += 1\n",
    "        success += 1\n",
    "    \n",
    "    if success == 4:\n",
    "        total_score += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Main score = \", round(main_score/len(results_df), 3))\n",
    "print(\"Sub score = \", round(sub_score/len(results_df), 3))\n",
    "print(\"Detail score = \", round(detail_score/len(results_df), 3))\n",
    "print(\"Level4 score = \", round(level4_score/len(results_df), 3))\n",
    "print(\"Total score = \", round(total_score/len(results_df), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
