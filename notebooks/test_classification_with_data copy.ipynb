{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced Image Classification with Item Data\n",
        "\n",
        "This notebook tests the hierarchical classification system on test_df.csv items using both images and item metadata (supplier_name, supplier_reference_description, materials).\n",
        "\n",
        "## Setup\n",
        "1. Make sure you have set your OpenRouter API key in `.env` file:\n",
        "   ```\n",
        "   OPENROUTER_API_KEY=your_api_key_here\n",
        "   ```\n",
        "2. The model is configured as `google/gemma-3-27b-it:free` in `app/config/settings.py`\n",
        "3. **Note**: This notebook will classify items even when images are not found, using metadata only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import base64\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.append(str(Path.cwd().parent))\n",
        "\n",
        "from app.langchain_modules.graph.classificationGraph import ClassificationGraphBuilder\n",
        "from app.langchain_modules.llm_definitions.openrouter_client import create_openrouter_client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "TEST_DF_PATH = \"../resources/test_df.csv\"\n",
        "PICS_AW_DIR = \"../pics/2026_AW\"\n",
        "PICS_SS_DIR = \"../pics/2026_SS\"\n",
        "\n",
        "# Load test dataframe\n",
        "test_df = pd.read_csv(TEST_DF_PATH)\n",
        "print(f\"Loaded {len(test_df)} items from test_df.csv\")\n",
        "print(f\"Columns: {test_df.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_image_for_item(item_id: int, pics_aw_dir: Path, pics_ss_dir: Path) -> Path | None:\n",
        "    \"\"\"\n",
        "    Find image file for a given item_id.\n",
        "    Searches in both 2026_AW and 2026_SS directories.\n",
        "    \n",
        "    Args:\n",
        "        item_id: Item ID to search for\n",
        "        pics_aw_dir: Path to 2026_AW directory\n",
        "        pics_ss_dir: Path to 2026_SS directory\n",
        "    \n",
        "    Returns:\n",
        "        Path to image file if found, None otherwise\n",
        "    \"\"\"\n",
        "    item_id_str = str(item_id)\n",
        "    \n",
        "    # Search in 2026_AW\n",
        "    for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n",
        "        pattern = f\"{item_id_str}*{ext}\"\n",
        "        matches = list(pics_aw_dir.rglob(pattern))\n",
        "        if matches:\n",
        "            # Return first match (prefer non-thumbnail if available)\n",
        "            non_thumbnails = [m for m in matches if 'thumbnail' not in str(m).lower()]\n",
        "            return non_thumbnails[0] if non_thumbnails else matches[0]\n",
        "    \n",
        "    # Search in 2026_SS\n",
        "    for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n",
        "        pattern = f\"{item_id_str}*{ext}\"\n",
        "        matches = list(pics_ss_dir.rglob(pattern))\n",
        "        if matches:\n",
        "            # Return first match (prefer non-thumbnail if available)\n",
        "            non_thumbnails = [m for m in matches if 'thumbnail' not in str(m).lower()]\n",
        "            return non_thumbnails[0] if non_thumbnails else matches[0]\n",
        "    \n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def image_to_base64(image_path: Path) -> str:\n",
        "    \"\"\"\n",
        "    Convert image file to base64 data URI.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to image file\n",
        "    \n",
        "    Returns:\n",
        "        Base64 data URI string\n",
        "    \"\"\"\n",
        "    with open(image_path, 'rb') as f:\n",
        "        image_data = base64.b64encode(f.read()).decode('utf-8')\n",
        "    \n",
        "    # Determine MIME type from extension\n",
        "    ext = image_path.suffix.lower()\n",
        "    mime_types = {\n",
        "        '.jpg': 'image/jpeg',\n",
        "        '.jpeg': 'image/jpeg',\n",
        "        '.png': 'image/png'\n",
        "    }\n",
        "    mime_type = mime_types.get(ext, 'image/jpeg')\n",
        "    \n",
        "    return f\"data:{mime_type};base64,{image_data}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_item_data(row: pd.Series) -> tuple:\n",
        "    \"\"\"\n",
        "    Extract item metadata from dataframe row.\n",
        "    \n",
        "    Args:\n",
        "        row: DataFrame row\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (supplier_name, supplier_reference_description, materials)\n",
        "        Returns None for missing values\n",
        "    \"\"\"\n",
        "    supplier_name = row.get('supplier_name')\n",
        "    supplier_reference_description = row.get('supplier_reference_description')\n",
        "    materials = row.get('materials')\n",
        "    \n",
        "    # Convert to None if NaN or empty string\n",
        "    supplier_name = None if pd.isna(supplier_name) or not str(supplier_name).strip() else str(supplier_name).strip()\n",
        "    supplier_reference_description = None if pd.isna(supplier_reference_description) or not str(supplier_reference_description).strip() else str(supplier_reference_description).strip()\n",
        "    materials = None if pd.isna(materials) or not str(materials).strip() else str(materials).strip()\n",
        "    \n",
        "    return supplier_name, supplier_reference_description, materials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize classification system\n",
        "print(\"Initializing classification system...\")\n",
        "llm = create_openrouter_client()\n",
        "graph_builder = ClassificationGraphBuilder(llm)\n",
        "graph_builder.build()\n",
        "print(\"Classification system ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare paths\n",
        "pics_aw_path = Path(PICS_AW_DIR)\n",
        "pics_ss_path = Path(PICS_SS_DIR)\n",
        "\n",
        "# Configuration for incremental saving\n",
        "CHECKPOINT_FILE = \"../resources/test_df_with_data_checkpoint.csv\"\n",
        "FINAL_OUTPUT_FILE = \"../resources/test_df_with_data_predictions.csv\"\n",
        "SAVE_EVERY_N_ITEMS = 5  # Save every 5 items processed\n",
        "\n",
        "# Check if checkpoint exists and load it\n",
        "checkpoint_path = Path(CHECKPOINT_FILE)\n",
        "RESUME_FROM_CHECKPOINT = True  # Set to False to start fresh\n",
        "\n",
        "if checkpoint_path.exists() and RESUME_FROM_CHECKPOINT:\n",
        "    print(f\"âœ“ Found checkpoint file: {CHECKPOINT_FILE}\")\n",
        "    print(\"Loading checkpoint to resume from previous run...\")\n",
        "    result_df = pd.read_csv(checkpoint_path)\n",
        "    already_processed = result_df['predicted_main'].notna().sum()\n",
        "    print(f\"âœ“ Loaded {len(result_df)} items from checkpoint\")\n",
        "    print(f\"âœ“ Items already processed: {already_processed}\")\n",
        "    print(f\"âœ“ Remaining items: {len(result_df) - already_processed}\")\n",
        "    print(\"  (Set RESUME_FROM_CHECKPOINT = False in the cell above to start fresh)\")\n",
        "else:\n",
        "    # Create a copy of test_df for results\n",
        "    result_df = test_df.copy()\n",
        "    \n",
        "    # Add new columns for classification results (use object dtype to avoid warnings)\n",
        "    result_df['predicted_main'] = None\n",
        "    result_df['predicted_sub'] = None\n",
        "    result_df['predicted_detail'] = None\n",
        "    result_df['predicted_level4'] = None\n",
        "    result_df['classification_errors'] = None\n",
        "    result_df['image_found'] = False\n",
        "\n",
        "# Convert to object dtype to avoid pandas warnings (only if columns exist)\n",
        "for col in ['predicted_main', 'predicted_sub', 'predicted_detail', 'predicted_level4', 'classification_errors']:\n",
        "    if col in result_df.columns:\n",
        "        result_df[col] = result_df[col].astype('object')\n",
        "\n",
        "print(f\"\\nResult dataframe ready with {len(result_df)} rows\")\n",
        "print(f\"Incremental saving: Every {SAVE_EVERY_N_ITEMS} items\")\n",
        "print(f\"Checkpoint file: {CHECKPOINT_FILE}\")\n",
        "print(f\"Final output file: {FINAL_OUTPUT_FILE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process each item with incremental saving\n",
        "total_items = len(result_df)\n",
        "successful = 0\n",
        "failed = 0\n",
        "no_image = 0\n",
        "skipped = 0  # Items already processed\n",
        "\n",
        "print(f\"Processing {total_items} items...\")\n",
        "print(\"This may take a while depending on API rate limits.\")\n",
        "print(f\"Results will be saved every {SAVE_EVERY_N_ITEMS} items to: {CHECKPOINT_FILE}\")\n",
        "print(\"Note: Items without images will be classified using metadata only.\\n\")\n",
        "\n",
        "processed_count = 0\n",
        "\n",
        "for idx, row in tqdm(result_df.iterrows(), total=total_items, desc=\"Classifying\"):\n",
        "    item_id = row['item_id']\n",
        "    \n",
        "    # Skip if already processed (has a prediction)\n",
        "    if pd.notna(result_df.at[idx, 'predicted_main']):\n",
        "        skipped += 1\n",
        "        continue\n",
        "    \n",
        "    # Extract item data\n",
        "    supplier_name, supplier_reference_description, materials = extract_item_data(row)\n",
        "    \n",
        "    # Find image for this item\n",
        "    image_path = find_image_for_item(item_id, pics_aw_path, pics_ss_path)\n",
        "    \n",
        "    if image_path is None:\n",
        "        result_df.at[idx, 'image_found'] = False\n",
        "        no_image += 1\n",
        "        image_base64 = None  # Will use metadata-only classification\n",
        "    else:\n",
        "        result_df.at[idx, 'image_found'] = True\n",
        "        # Convert image to base64\n",
        "        image_base64 = image_to_base64(image_path)\n",
        "    \n",
        "    # Always call classify() - with image if available, or metadata-only if not\n",
        "    try:\n",
        "        # Run classification with retry logic for rate limits\n",
        "        max_retries = 5\n",
        "        base_delay = 30  # Start with 30 seconds (more conservative)\n",
        "        result = None\n",
        "        last_error = None\n",
        "        \n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                result = graph_builder.classify(\n",
        "                    image_data=image_base64,\n",
        "                    supplier_name=supplier_name,\n",
        "                    supplier_reference_description=supplier_reference_description,\n",
        "                    materials=materials\n",
        "                )\n",
        "                \n",
        "                # Check if result has critical errors even if no exception was raised\n",
        "                if result.errors:\n",
        "                    error_str = '; '.join(result.errors)\n",
        "                    is_rate_limit = '429' in error_str or 'rate limit' in error_str.lower() or 'rate-limited' in error_str.lower()\n",
        "                    is_400_error = '400' in error_str or 'Developer instruction' in error_str\n",
        "                    \n",
        "                    # If critical error and not last attempt, retry\n",
        "                    if (is_rate_limit or is_400_error) and attempt < max_retries - 1:\n",
        "                        wait_time = base_delay * (2 ** attempt)\n",
        "                        error_type = \"Rate limit\" if is_rate_limit else \"System message\"\n",
        "                        tqdm.write(f\"âš ï¸  {error_type} error in result for item {item_id}, waiting {wait_time}s before retry {attempt + 1}/{max_retries}\")\n",
        "                        time.sleep(wait_time)\n",
        "                        result = None  # Reset to retry\n",
        "                        continue\n",
        "                    # If main classification failed, that's critical\n",
        "                    elif not result.main and any('MainClassifier' in e for e in result.errors):\n",
        "                        if attempt < max_retries - 1:\n",
        "                            wait_time = base_delay * (2 ** attempt)\n",
        "                            tqdm.write(f\"âš ï¸  Main classification failed for item {item_id}, waiting {wait_time}s before retry {attempt + 1}/{max_retries}\")\n",
        "                            time.sleep(wait_time)\n",
        "                            result = None\n",
        "                            continue\n",
        "                \n",
        "                # If we got here and have a result (even with minor errors), break\n",
        "                if result:\n",
        "                    break\n",
        "                    \n",
        "            except Exception as e:\n",
        "                last_error = e\n",
        "                error_str = str(e)\n",
        "                # Check if it's a rate limit error or 400 error (system message issue)\n",
        "                is_rate_limit = '429' in error_str or 'rate limit' in error_str.lower() or 'rate-limited' in error_str.lower()\n",
        "                is_400_error = '400' in error_str or 'Developer instruction' in error_str\n",
        "                \n",
        "                if (is_rate_limit or is_400_error) and attempt < max_retries - 1:\n",
        "                    # Exponential backoff: 30s, 60s, 120s, 240s, 480s\n",
        "                    wait_time = base_delay * (2 ** attempt)\n",
        "                    error_type = \"Rate limit\" if is_rate_limit else \"System message\"\n",
        "                    tqdm.write(f\"âš ï¸  {error_type} error for item {item_id}, waiting {wait_time}s before retry {attempt + 1}/{max_retries}\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "                # If not retryable error or last attempt, break to handle error\n",
        "                break\n",
        "        \n",
        "        if result and result.main:  # Only count as successful if we got at least main\n",
        "            # Store results\n",
        "            result_df.at[idx, 'predicted_main'] = result.main\n",
        "            result_df.at[idx, 'predicted_sub'] = result.sub if result.sub else None\n",
        "            result_df.at[idx, 'predicted_detail'] = result.detail if result.detail else None\n",
        "            result_df.at[idx, 'predicted_level4'] = result.level4 if result.level4 else None\n",
        "            \n",
        "            if result.errors:\n",
        "                result_df.at[idx, 'classification_errors'] = '; '.join(result.errors)\n",
        "            \n",
        "            successful += 1\n",
        "        else:\n",
        "            # Classification failed after all retries\n",
        "            error_msg = str(last_error) if last_error else \"Classification failed after retries\"\n",
        "            if result and result.errors:\n",
        "                error_msg = '; '.join(result.errors)\n",
        "            result_df.at[idx, 'classification_errors'] = error_msg\n",
        "            failed += 1\n",
        "            tqdm.write(f\"âŒ Failed after {max_retries} retries for item {item_id}\")\n",
        "        \n",
        "        processed_count += 1\n",
        "        \n",
        "        # Add delay to respect rate limits (free tier: 16 requests/min = ~3.75s between requests)\n",
        "        # But we're doing 4 API calls per item (main, sub, detail, level4), so need more delay\n",
        "        # 16 requests/min = 1 request per 3.75s, but we do 4 requests = need ~15s between items\n",
        "        time.sleep(15.0)  # More conservative delay for free tier (4 API calls per item)\n",
        "        \n",
        "    except Exception as e:\n",
        "        result_df.at[idx, 'classification_errors'] = str(e)\n",
        "        failed += 1\n",
        "        processed_count += 1\n",
        "        error_msg = str(e)\n",
        "        if '429' in error_msg or 'rate limit' in error_msg.lower():\n",
        "            tqdm.write(f\"âŒ Rate limit error for item {item_id}, will retry on next run\")\n",
        "        else:\n",
        "            tqdm.write(f\"âŒ Error processing item {item_id}: {error_msg[:100]}\")\n",
        "    \n",
        "    # Save checkpoint every N items\n",
        "    if processed_count > 0 and processed_count % SAVE_EVERY_N_ITEMS == 0:\n",
        "        result_df.to_csv(CHECKPOINT_FILE, index=False)\n",
        "        tqdm.write(f\"ðŸ’¾ Checkpoint saved: {processed_count} items processed, \"\n",
        "                  f\"{successful} successful, {failed} failed, {no_image} no image\")\n",
        "\n",
        "# Final save\n",
        "result_df.to_csv(CHECKPOINT_FILE, index=False)\n",
        "result_df.to_csv(FINAL_OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"\\n\\n{'='*60}\")\n",
        "print(f\"Classification complete!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total items: {total_items}\")\n",
        "print(f\"Successful: {successful}\")\n",
        "print(f\"Failed: {failed}\")\n",
        "print(f\"No image found (metadata-only): {no_image}\")\n",
        "print(f\"Skipped (already processed): {skipped}\")\n",
        "print(f\"\\nResults saved to:\")\n",
        "print(f\"  - Checkpoint: {CHECKPOINT_FILE}\")\n",
        "print(f\"  - Final output: {FINAL_OUTPUT_FILE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper: Clear errors for failed items\n",
        "\n",
        "Run this cell if you want to retry items that failed. This will clear the `predicted_main` value for items that have errors, allowing them to be retried.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear errors for failed items to allow retry\n",
        "if 'classification_errors' in result_df.columns:\n",
        "    failed_mask = result_df['classification_errors'].notna()\n",
        "    failed_count = failed_mask.sum()\n",
        "    if failed_count > 0:\n",
        "        print(f\"Found {failed_count} items with errors.\")\n",
        "        result_df.loc[failed_mask, 'predicted_main'] = None\n",
        "        result_df.loc[failed_mask, 'predicted_sub'] = None\n",
        "        result_df.loc[failed_mask, 'predicted_detail'] = None\n",
        "        result_df.loc[failed_mask, 'predicted_level4'] = None\n",
        "        result_df.loc[failed_mask, 'classification_errors'] = None\n",
        "        result_df.to_csv(CHECKPOINT_FILE, index=False)\n",
        "        print(f\"âœ“ Cleared predictions for {failed_count} items. They will be retried on next run.\")\n",
        "    else:\n",
        "        print(\"No items with errors found.\")\n",
        "else:\n",
        "    print(\"No classification_errors column found.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
